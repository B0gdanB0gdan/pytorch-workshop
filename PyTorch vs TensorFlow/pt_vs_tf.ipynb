{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447618ac-ff6f-4aeb-addf-b290f1cbb8d7",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Compare different aspects of the two frameworks in terms of:\n",
    "* readability (e.g., number of lines, what patterns they follow)\n",
    "* performance (e.g., loss, acc.)\n",
    "* speed: data loading, training (forward+backward passes), inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc4fc0-935e-47cb-86aa-6fee64071a72",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bbf2d-0d3d-4319-a12a-a90890b4e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, metrics, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd142a0-c54d-47de-9f49-6d06b577e54e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba032f29-7778-431b-826b-91b3fbdee564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c796f-81e0-4a60-80dd-637368f742ad",
   "metadata": {},
   "source": [
    "Check if CUDA is installed properly for both frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23434f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(th.cuda.is_available())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7fb40-9668-4b78-9b85-83fcd142573b",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9f408-21ad-4360-ad9c-6cbf66880fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "SEED = 17\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730ae3e-a4f2-491f-8b04-5661795ef8ca",
   "metadata": {},
   "source": [
    "# Convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd23a0-8565-4989-a76b-dd2930dfc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data structure to hold different datasets, models and results\n",
    "# For the purpose of this notebook only a CNN is analysed \n",
    "\n",
    "config = {\n",
    "    \"th\": {\n",
    "        \"datasets\": {},\n",
    "        \"models\": {}\n",
    "    },\n",
    "    \"tf\": {\n",
    "        \"datasets\": {},\n",
    "        \"models\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d2418-351a-40db-aad9-991bc38d54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8f441-ba47-45cc-9b20-8fee9f21b5ba",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa14c51-1755-46a1-a344-391c329cc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (val_images, val_labels) = datasets.cifar10.load_data()\n",
    "# can do transforms but with TFX library or implement as tf layers (that have the advantage to be part of the model i.e. can be used also at inference)\n",
    "train_images, val_images = (train_images - 0.5) / 0.5, (val_images - 0.5) / 0.5\n",
    "\n",
    "# from_tensor_slices splits the input data along first dimension\n",
    "# A tf.data.Dataset represents a sequence of elements where element = single data point (e.g., feature-label pair) | batch of data points\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# keep in mind that setting the buffer_size=card(dataset) will load all the training data into memory\n",
    "train_loader = train_dataset.shuffle(buffer_size=train_dataset.cardinality()).batch(BATCH_SIZE)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_loader = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "print(len(train_images), len(val_labels), train_dataset.element_spec)\n",
    "\n",
    "config[\"tf\"][\"datasets\"][\"cifar10\"] = dict(train_dataset=train_dataset,\n",
    "                                          val_dataset=val_dataset,\n",
    "                                          train_loader=train_loader, \n",
    "                                          val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5886b-59cc-49a1-b27f-0d3f9791e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# num_workers=0 for Windows\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "train_loader = th.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True, num_workers=0)\n",
    "val_loader = th.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                     shuffle=False, num_workers=0)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(len(train_dataset), len(val_dataset), train_dataset[0][0].shape, train_dataset[0][1])\n",
    "print(class_names)\n",
    "\n",
    "config[\"th\"][\"datasets\"][\"cifar10\"] = dict(train_dataset=train_dataset,\n",
    "                                          val_dataset=val_dataset,\n",
    "                                          train_loader=train_loader, \n",
    "                                          val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278f048-64fb-4841-9b2b-4e9b3b2a5295",
   "metadata": {},
   "source": [
    "### Conclusions so far\n",
    "* TF is channels-last while TH is channel-first\n",
    "* Both dataset implementations use lazy loading \n",
    "* Both can parallelize loading and processing\n",
    "* Both have sparse labels => SparseCategoricalCrossEntropy\n",
    "* PyTorch has builtin transforms for data augmentations while TensorFlow has to rely direclty on python or other libraries (e.g., TFX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291214a4-ff8c-4f8b-ab89-fda9936e8206",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c34aa1-e855-498d-950b-44159155279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_CNN(models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')\n",
    "        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv2 = layers.Conv2D(32, (3, 3), activation='relu')\n",
    "        self.pool2 = layers.MaxPooling2D((2, 2))\n",
    "        self.conv3 = layers.Conv2D(64, (3, 3), activation='relu')\n",
    "        self.flatten = layers.Flatten() # 4 * 4 * 64\n",
    "        self.fc1 = layers.Dense(64, activation='relu')\n",
    "        self.fc2 = layers.Dense(10)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "model = TF_CNN()\n",
    "model.build(input_shape=(BATCH_SIZE, 32, 32, 3))\n",
    "model.summary()\n",
    "config[\"tf\"][\"models\"][\"cnn\"] = {\"model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c406e2d-1b56-4e48-8a6d-f9cace1f2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TH_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*4*4, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = th.flatten(x, 1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = TH_CNN()\n",
    "print(f\"Number of trainable paramters PT: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "config[\"th\"][\"models\"][\"cnn\"] = {\"model\": model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383dc16-a14a-4ae8-a21a-e2ed50ff4d6c",
   "metadata": {},
   "source": [
    "### Conclusions so far\n",
    "* Both can use Sequential and Functional APIs\n",
    "* tensorflow developers would assume feature dimensions and make only channel dimensions explicit\n",
    "* pytorch developers will need to make each dimension explicit\n",
    "* pytorch conv layers expect channels first while tensorflow channel last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a7932-c313-49cf-b5ce-743b0dca4547",
   "metadata": {},
   "source": [
    "# Criterion, metrics & optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92e8db-3999-48ac-b15e-9c86c5ae2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sce_tf = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "adam_tf = optimizers.Adam(learning_rate=LR)\n",
    "accuracy_tf = metrics.Accuracy()\n",
    "\n",
    "config[\"tf\"][\"models\"][\"cnn\"][\"criterions\"] = {\"sce\": sce_tf}\n",
    "config[\"tf\"][\"models\"][\"cnn\"][\"optimizers\"] = {\"adam\": adam_tf}\n",
    "config[\"tf\"][\"models\"][\"cnn\"][\"metrics\"] = {\"acc\": accuracy_tf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2391e6-68ef-45c0-aaca-7c66f0728994",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_th = nn.CrossEntropyLoss()\n",
    "adam_th = optim.Adam(config[\"th\"][\"models\"][\"cnn\"][\"model\"].parameters(), lr=LR)\n",
    "\n",
    "def accuracy_th(logits, targs):\n",
    "    pred_classes = th.argmax(logits, -1)\n",
    "    correct = (pred_classes == targs).sum().item()\n",
    "    return correct / len(targs)\n",
    "\n",
    "config[\"th\"][\"models\"][\"cnn\"][\"criterions\"] = {\"ce\": ce_th}\n",
    "config[\"th\"][\"models\"][\"cnn\"][\"optimizers\"] = {\"adam\": adam_th}\n",
    "config[\"th\"][\"models\"][\"cnn\"][\"metrics\"] = {\"acc\": accuracy_th}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57330434-2fc8-4a47-9249-df43a0d72c9a",
   "metadata": {},
   "source": [
    "### Conclusions so far\n",
    "* CategoricalCrossEntropy expects targets to be one-hot encoded, from_logits=True means no Softmax in the last layer\n",
    "* SparseCategoricalCrossEntropy expects targets to be integer indices, from_logits=True means no Softmax in the last layer\n",
    "* CrossEntropyLoss always expects logits as predictions and targets as either integers or one hot encodings\n",
    "* CrossEntropyLoss <=> LogSoftmax + NLLLoss\n",
    "* No metrics, rely on Python or external library e.g., torchmetrics, sklearn\n",
    "* optimizer in Pytorch expects model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d3287-a761-4b60-9325-38e63b70fd5f",
   "metadata": {},
   "source": [
    "# Train loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2013949-937c-42ed-8a47-a1ae96ddf085",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb45a4-c2c0-483f-845e-05e781bdf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove annotation to see performance impact\n",
    "@tf.function\n",
    "def train_step_tf(model, inps, targs, criterion, optimizer):\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies to its inputs are going to be recorded on the GradientTape.\n",
    "        preds = model(inps, training=True) \n",
    "        loss_value = criterion(targs, preds)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "def train_tf(model, train_loader, val_loader, criterion, optimizer):\n",
    "    for epoch in range(NUM_EPOCHS):    \n",
    "        train_loss = 0.0       \n",
    "        for step, (inps, targs) in enumerate(train_loader):\n",
    "            tloss = train_step_tf(model, inps, targs, criterion, optimizer)\n",
    "            train_loss += float(tloss)\n",
    "            \n",
    "        val_loss = 0.0     \n",
    "        for val_inps, val_targs in val_loader:\n",
    "            val_preds = model(val_inps, training=False)\n",
    "            vloss = criterion(val_targs, val_preds)\n",
    "            val_loss += float(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch} \\t Train loss {train_loss/len(train_loader)} \\t Val loss {val_loss/len(val_loader)}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_tf(model, data_loader, metric, criterion):\n",
    "    criterion_res, metric_res= 0.0, 0.0\n",
    "    for inp, targ in data_loader:\n",
    "        pred = model(inp, training=False)\n",
    "        \n",
    "        metric.update_state(targ, tf.argmax(pred, -1)[..., None])\n",
    "        criterion_res += float(criterion(targ, pred))\n",
    "        metric_res += metric.result().numpy()\n",
    "        metric.reset_state()\n",
    "    criterion_res /= len(data_loader)\n",
    "    metric_res /= len(data_loader)\n",
    "    return metric_res, criterion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745d657-bdb7-4e03-9a9a-aa287183cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_tf(model=config[\"tf\"][\"models\"][\"cnn\"][\"model\"], \n",
    "        train_loader=config[\"tf\"][\"datasets\"][\"cifar10\"][\"train_loader\"],\n",
    "        val_loader=config[\"tf\"][\"datasets\"][\"cifar10\"][\"val_loader\"],\n",
    "        criterion=config[\"tf\"][\"models\"][\"cnn\"][\"criterions\"][\"sce\"],\n",
    "        optimizer=config[\"tf\"][\"models\"][\"cnn\"][\"optimizers\"][\"adam\"])\n",
    "\n",
    "acc_tf, loss_tf = evaluate_tf(model=model,\n",
    "                  data_loader=config[\"tf\"][\"datasets\"][\"cifar10\"][\"val_loader\"],\n",
    "                  metric=config[\"tf\"][\"models\"][\"cnn\"][\"metrics\"][\"acc\"],\n",
    "                  criterion=config[\"tf\"][\"models\"][\"cnn\"][\"criterions\"][\"sce\"])\n",
    "print(acc_tf, loss_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21658312-bce6-4aa4-b3e4-3d0583b4dee0",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca16148-a42f-4303-a44e-57d0449461d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_th(model, inps, targs, criterion, optimizer):\n",
    "    # zero the parameter gradients to avoid leaking previous model performance\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    preds = model(inps)\n",
    "    tloss = criterion(preds, targs)\n",
    "    \n",
    "    tloss.backward()\n",
    "    optimizer.step()\n",
    "    return tloss\n",
    "\n",
    "def train_th(model, train_loader, val_loader, criterion, optimizer):\n",
    "    model.to(device)\n",
    "    for epoch in range(NUM_EPOCHS): \n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inps, targs = batch[0].to(device), batch[1].to(device)\n",
    "            tloss = train_step_th(model, inps, targs, criterion, optimizer)\n",
    "            train_loss += tloss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with th.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                val_inps, val_targs = val_batch[0].to(device), val_batch[1].to(device)\n",
    "                val_preds = model(val_inps)\n",
    "                vloss = criterion(val_preds, val_targs)\n",
    "                val_loss += vloss.item()                    \n",
    "        print(f\"Epoch {epoch} \\t Train loss {train_loss/len(train_loader)} \\t Val loss {val_loss/len(val_loader)}\")\n",
    "    return model\n",
    "    \n",
    "def evaluate_th(model, data_loader, metric, criterion):\n",
    "    model.eval()\n",
    "    metric_res, criterion_res = 0.0, 0.0\n",
    "    for inps, targs in data_loader:\n",
    "        inps = inps.to(device)\n",
    "        targs = targs.to(device)\n",
    "        model.to(device)\n",
    "        with th.no_grad():\n",
    "            preds = model(inps)\n",
    "        metric_res += metric(preds, targs)\n",
    "        criterion_res += criterion(preds, targs).item()\n",
    "        \n",
    "    metric_res /= len(data_loader)    \n",
    "    criterion_res /= len(data_loader)    \n",
    "    return metric_res, criterion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3f173-9f98-41a2-8615-bc31647064d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_th(model=config[\"th\"][\"models\"][\"cnn\"][\"model\"], \n",
    "        train_loader=config[\"th\"][\"datasets\"][\"cifar10\"][\"train_loader\"],\n",
    "        val_loader=config[\"th\"][\"datasets\"][\"cifar10\"][\"val_loader\"],\n",
    "        criterion=config[\"th\"][\"models\"][\"cnn\"][\"criterions\"][\"ce\"],\n",
    "        optimizer=config[\"th\"][\"models\"][\"cnn\"][\"optimizers\"][\"adam\"])\n",
    "acc_th, loss_th = evaluate_th(model=model,\n",
    "                  data_loader=config[\"th\"][\"datasets\"][\"cifar10\"][\"val_loader\"],\n",
    "                  metric=config[\"th\"][\"models\"][\"cnn\"][\"metrics\"][\"acc\"],\n",
    "                  criterion=config[\"th\"][\"models\"][\"cnn\"][\"criterions\"][\"ce\"])\n",
    "print(acc_th, loss_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f7c27-5d54-4be3-b8b7-b8de236028c3",
   "metadata": {},
   "source": [
    "# Let's compare\n",
    "* parameters, loading speed, fw+bw pass, inf\n",
    "* how fast is on cuda\n",
    "* how fast is on cuda+cpu(transfer)\n",
    "* PyTorch has direct access to cuda events but TensorFlow uses a profiler so we cannot make a fair comparison => CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93558f8-9a69-4143-bf77-b581588ff594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# use CuPy to measure GPU time\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bea2b9-62db-4b94-9a1c-318021e88136",
   "metadata": {},
   "source": [
    "### Define more or less the same test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98223e-5fc6-4336-9cd9-d4b6795b7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inps_th = th.randn((BATCH_SIZE, 3, 32, 32), dtype=th.float32)\n",
    "targs_th = th.randint(0, 10, (BATCH_SIZE,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4555f-7e58-4eb7-92d8-524b4d970a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inps_tf = tf.random.normal((BATCH_SIZE, 32, 32, 3), dtype=tf.float32)\n",
    "targs_tf = tf.random.uniform(shape=(BATCH_SIZE,), minval=0, maxval=10, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f913d2e-1c3e-48ca-8d8b-f7e1b3ce20da",
   "metadata": {},
   "source": [
    "### Train step time (CPU + GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82591c3d-7d9a-4e84-b484-4be2b7f11817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_train_step_time_th(model, inps, targs, criterion, optimizer, n=10000):\n",
    "    avg_cpu_time, avg_gpu_time = 0.0, 0.0 \n",
    "    start_gpu = cp.cuda.Event()\n",
    "    end_gpu = cp.cuda.Event()\n",
    "\n",
    "    for _ in range(n):\n",
    "        # time measures wall-clock time that includes both CPU and GPU times\n",
    "        start_gpu.record()\n",
    "        start_cpu = time.perf_counter()\n",
    "\n",
    "        # As TensorFlow handles the data migration on itself we have to include this for a fair comparison\n",
    "        model.to(device)\n",
    "        inps = inps.to(device)\n",
    "        targs = targs.to(device)\n",
    "       \n",
    "        train_step_th(model, inps, targs, criterion, optimizer)\n",
    "        \n",
    "        end_cpu = time.perf_counter()\n",
    "        end_gpu.record()\n",
    "        end_gpu.synchronize() # GPU is running async\n",
    "        \n",
    "        avg_cpu_time += end_cpu-start_cpu\n",
    "        avg_gpu_time += cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "        \n",
    "    avg_gpu_time /= n\n",
    "    avg_cpu_time /= n\n",
    "    return avg_cpu_time/1000, avg_gpu_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3787642-9e5b-4f47-b85e-95047234c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_train_step_time_tf(model, inps, targs, criterion, optimizer, n=10000):\n",
    "    avg_cpu_time, avg_gpu_time = 0.0, 0.0 \n",
    "    start_gpu = cp.cuda.Event()\n",
    "    end_gpu = cp.cuda.Event()\n",
    "\n",
    "    for _ in range(n):\n",
    "        # time measures wall-clock time that includes both CPU and GPU times\n",
    "        start_gpu.record()\n",
    "        start_cpu = time.perf_counter()\n",
    "        \n",
    "        train_step_tf(model, inps, targs, criterion, optimizer)\n",
    "        \n",
    "        end_cpu = time.perf_counter()\n",
    "        end_gpu.record()\n",
    "        end_gpu.synchronize() # GPU is running async\n",
    "        \n",
    "        avg_cpu_time += end_cpu-start_cpu\n",
    "        avg_gpu_time += cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "        \n",
    "    avg_gpu_time /= n\n",
    "    avg_cpu_time /= n\n",
    "    return avg_cpu_time/1000, avg_gpu_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1ec07-af05-496c-af4f-abcfd8231ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_time_th, gpu_time_th = measure_train_step_time_th(model=config[\"th\"][\"models\"][\"cnn\"][\"model\"], \n",
    "                                 inps=inps_th, \n",
    "                                 targs=targs_th,\n",
    "                                 criterion=config[\"th\"][\"models\"][\"cnn\"][\"criterions\"][\"ce\"],\n",
    "                                 optimizer=config[\"th\"][\"models\"][\"cnn\"][\"optimizers\"][\"adam\"])\n",
    "\n",
    "cpu_time_tf, gpu_time_tf = measure_train_step_time_tf(model=config[\"tf\"][\"models\"][\"cnn\"][\"model\"], \n",
    "                                 inps=inps_tf, \n",
    "                                 targs=targs_tf,\n",
    "                                 criterion=config[\"tf\"][\"models\"][\"cnn\"][\"criterions\"][\"sce\"],\n",
    "                                 optimizer=config[\"tf\"][\"models\"][\"cnn\"][\"optimizers\"][\"adam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a68f46-2c57-44a5-8fe0-04dbf6af7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cpu_time_th, gpu_time_th)\n",
    "print(cpu_time_tf, gpu_time_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b69992-e5b4-41e5-b712-faa5bae5a598",
   "metadata": {},
   "source": [
    "### Batch Loading Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ed07f-5304-4398-98f9-452f61e252fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_loading_speed(loader):\n",
    "    avg_load_time = 0.0\n",
    "    start_cpu = time.perf_counter()\n",
    "    for batch in loader:\n",
    "        batch\n",
    "    end_cpu = time.perf_counter()\n",
    "    avg_load_time = (end_cpu - start_cpu) / len(loader)\n",
    "    return avg_load_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0d8de-5b61-42f9-98d5-3aecd79e08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_time_th = get_batch_loading_speed(config[\"th\"][\"datasets\"][\"cifar10\"][\"val_loader\"])\n",
    "batch_time_tf = get_batch_loading_speed(config[\"tf\"][\"datasets\"][\"cifar10\"][\"val_loader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce64b7-35a9-43f4-8db1-9e3effed6585",
   "metadata": {},
   "source": [
    "### Model Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252826d-1b2a-4244-b5a8-17b186d6ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_time_th(model, inp, n=10000):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    inp = inp.to(device)\n",
    "    start_gpu = cp.cuda.Event()\n",
    "    end_gpu = cp.cuda.Event()\n",
    "    avg_elapsed_time = 0.0\n",
    "    for _ in range(n):\n",
    "        start_gpu.record()\n",
    "        with th.no_grad():\n",
    "            model(inp)\n",
    "        end_gpu.record()\n",
    "        end_gpu.synchronize() # GPU is running async\n",
    "        avg_elapsed_time += cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "    return avg_elapsed_time/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc03cd8-f722-4f6f-bfec-89c0ed2c1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_model(model, inp):\n",
    "    return model(inp)\n",
    "\n",
    "def get_inference_time_tf(model, inp, n=10000):\n",
    "    start_gpu = cp.cuda.Event()\n",
    "    end_gpu = cp.cuda.Event()\n",
    "    avg_elapsed_time = 0.0\n",
    "    for _ in range(n):\n",
    "        start_gpu.record()\n",
    "        run_model(model, inp)\n",
    "        end_gpu.record()\n",
    "        end_gpu.synchronize() # GPU is running async\n",
    "        avg_elapsed_time += cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "    return avg_elapsed_time/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c774d-5c83-4f31-ac58-cd5ff0d0649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_gpu_time_th = get_inference_time_th(model=config[\"th\"][\"models\"][\"cnn\"][\"model\"], inp = inps_th)\n",
    "inf_gpu_time_tf = get_inference_time_tf(model=config[\"tf\"][\"models\"][\"cnn\"][\"model\"], inp=inps_tf)\n",
    "print(inf_gpu_time_th, inf_gpu_time_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c4f3c-25cd-4787-9156-82e3c8b9a777",
   "metadata": {},
   "source": [
    "# Final Results\n",
    "Please run all the above cells to populate the following pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c959451-65f2-458b-9e51-f588eac7a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cnn_data = {'Train step time CPU [ms]': [cpu_time_th, cpu_time_tf], \n",
    "            'Train step time GPU [ms]': [gpu_time_th, gpu_time_tf],\n",
    "            'Inference time GPU [ms]': [inf_gpu_time_th, inf_gpu_time_tf],\n",
    "            'Batch loading speed [ms]': [batch_time_th, batch_time_tf],\n",
    "            'CE Loss': [loss_th, loss_tf],\n",
    "            'Val Accuracy': [acc_th, acc_tf]}\n",
    "df = pd.DataFrame(cnn_data, index=['PyTorch', 'TensorFlow'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071ead6-f31c-41f2-a81b-db02d3609652",
   "metadata": {},
   "source": [
    "### Exercise: Compare both frameworks for RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852dbe56-9915-4770-a9fe-9e9f37e1e368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calib_transf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
