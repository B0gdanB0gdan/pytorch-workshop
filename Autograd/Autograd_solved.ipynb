{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2907ab42",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Understand how Autograd enables us to train neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15e4f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d1c15-80c4-43c2-bfd6-c040d6e6f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab258d9f",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "<b> In a nutshell: </b>\n",
    "* feature of PyTorch that allows computation of partial derivatives (gradients)\n",
    "* central to backpropagation <br>\n",
    "\n",
    "<b> Training a NN: </b>\n",
    "* View a NN as a collection of nested functions <br>\n",
    "<b> Forward pass </b>\n",
    "* NN makes its best guess about the outputs\n",
    "* Data flows through each of these functions to compute the output <br>\n",
    "<b> Backward pass </b>\n",
    "* NN adjust its parameters\" in the direction of minimizing the loss\n",
    "* A gradient = vector (direction, magnitude) that points in the direction of steepest increase in the loss function\n",
    "* Collects gradients starting from the end <br>\n",
    "![gradient_descent.png](../Presentations/assets_autograd/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664b8ec",
   "metadata": {},
   "source": [
    "Q: How PyTorch adjust the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01522a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General setup\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "BATCH_SIZE, CHANNELS, IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES = 32, 3, 64, 64, 1000\n",
    "data = th.rand(BATCH_SIZE, CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
    "targ = th.rand(BATCH_SIZE, NUM_CLASSES)\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "\n",
    "model = resnet18(weights=weights)\n",
    "optim = th.optim.SGD(model.parameters(), lr=1e-2)\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "objective = th.nn.CrossEntropyLoss()\n",
    "\n",
    "model.to(device)\n",
    "data = data.to(device)\n",
    "targ = targ.to(device)\n",
    "\n",
    "pred = model(data)  # forward pass\n",
    "loss = objective(pred, targ)  # evaluate guess\n",
    "initial_weight = model.fc.weight[0][0].item()\n",
    "loss.backward()   # backward pass\n",
    "grad_value = model.fc.weight.grad[0][0].item()\n",
    "optim.step()  # adjust weights\n",
    "updated_weight = model.fc.weight[0][0].item()\n",
    "print(initial_weight, grad_value, updated_weight, initial_weight - 1e-2*grad_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f4bf1d6",
   "metadata": {},
   "source": [
    "### Computation graphs\n",
    "![computation_graph.png](../Presentations/assets_autograd/computation_graph.png)\n",
    "\n",
    "* During forward propagation the (conceptual) graph is augmented ==> the real graph is created\n",
    "* Each node has a context (saves inputs and outputs of the function)\n",
    "* Specifically the graph (DAG) is constructed from torch.autograd.Function(s) and tensors(with requires_grad=True, also called leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = th.tensor([1.0], requires_grad=True)  # leaf node because they are not the result of any computation\n",
    "x2 = th.tensor([2.0], requires_grad=True)\n",
    "a = x1 * x2\n",
    "y1 = th.log(a)\n",
    "y2 = th.sin(x2)\n",
    "w = y1 * y2\n",
    "z = w\n",
    "\n",
    "print(x1.grad_fn) # can't backpropagate beyond leaf nodes\n",
    "print(z.grad_fn)\n",
    "print(isinstance(z.grad_fn, th.autograd.graph.Node))\n",
    "print(dir(z.grad_fn))\n",
    "print(y1, y2, z.grad_fn._saved_other, z.grad_fn._saved_self)\n",
    "\n",
    "make_dot(z, params={\"x1\":x1, \"x2\":x2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b00fc",
   "metadata": {},
   "source": [
    "### Differentiation\n",
    "![tracking.png](../Presentations/assets_autograd/tracking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe02aa4",
   "metadata": {},
   "source": [
    "Q: How Autograd collects gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a67e30",
   "metadata": {},
   "source": [
    "Autograd traces computation dynamically at runtime e.g., model implies ifs, fors (length not known). <br>\n",
    "\n",
    "* Every computed tensor tracks a history of its inputs and the function used to create it\n",
    "* Every such function has a built-in implementation to compute its own derivatives (Add --> AddBackward)\n",
    "* Setting requires_grad = True on a tensor -> Track every computation that follows BUT in the output tensors i.e. <b>requires_grad=True is contagious</b>; y is a computation on x so y will track its computation history\n",
    "* SinBackward tells us that during backprop step weâ€™ll need to compute the sin derivative of the output during fw step wrt the inputs of this function\n",
    "* Gradients of a tensor are accumulated for every bw pass --> optimizer.zero_grad()\n",
    "* Calling backward will destroy the graph => calling a second time will result in error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1., 2.])\n",
    "print(x)\n",
    "x = th.tensor([1., 2.], requires_grad=True)\n",
    "print(x)\n",
    "y = th.sin(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0cf05",
   "metadata": {},
   "source": [
    "Each grad_fn stored in within the tensors allows us to track the computation all the way back to its inputs using <b>next_functions</b> property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd832076",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1., 2.])\n",
    "print(x)\n",
    "x = th.tensor([1., 2.], requires_grad=True)\n",
    "y = x**2 + 1\n",
    "z = 3*y\n",
    "print(z.grad_fn)\n",
    "print(z.grad_fn.next_functions)\n",
    "print(z.grad_fn.next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c00fac",
   "metadata": {},
   "source": [
    "Q: How efficient is Autograd?\n",
    "\n",
    "When training a neural network we need to compute gradients of a loss function w.r.t. every weight and bias.\n",
    "If we were to expand the expression $\\nabla_WL$ using the chain rule it will result in many partial derivatives over every weight, activation function and other mathematical transformation. Each such partial derivative is the sum of the products of the local gradients of every possible path through the computation graph that leads to the variable whose gradient we are trying to measure => number of partial derivatives will tend to go exponentially with the depth of the network.\n",
    "\n",
    "![paths.png](../Presentations/assets_autograd/paths.png)\n",
    "\n",
    "For efficient computation of the gradients, Autograd combines two powerful concepts: \n",
    "1) Jacobian-vector products to propagate gradients  \n",
    "2) Each tensor tracks history of its inputs and the function used to create it (each such function has a built-in implementation for its own derivatives).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18e58e",
   "metadata": {},
   "source": [
    "Once we call <b>.backward()</b> on L, we get $dL/dL=1$ => invoke its backward function (compute the gradient of the output of the Function object w.r.t to the inputs of the Function object i.e. $dL/dy$). This computed gradient is multiplied by the accumulated gradient (stored in the <b>.grad</b> of the current node which is $acc=dL/dL=1$) and then sent to the input node. What we have done was to apply chain rule on the path L-> y. \n",
    "\n",
    "![jaccobian.png](../Presentations/assets_autograd/jaccobian.png) <br>\n",
    "\n",
    "Note: By default only on leaf nodes we can access the gradient value => call <b>.retain_grad()</b> just after declaring it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = th.tensor([[2., 4.], [1., 0.], [2., 0.5]], requires_grad=True)\n",
    "b = th.tensor([1.0, 1.0, 1.0], requires_grad=True)\n",
    "\n",
    "x = th.randn(2)\n",
    "\n",
    "f = lambda W, b: x @ W.t() + b\n",
    "\n",
    "y = f(W, b)  # function f\n",
    "y.retain_grad()\n",
    "\n",
    "y_t = th.tensor([0.3, 0.4, 0.3])\n",
    "\n",
    "l = lambda y: ((y-y_t)**2).mean() # l=mse\n",
    "\n",
    "e = l(y) \n",
    "e.retain_grad()\n",
    "\n",
    "e.backward()\n",
    "\n",
    "acc_grad = e.grad\n",
    "\n",
    "print(y.grad.equal(th.autograd.functional.jacobian(l, y) * acc_grad))\n",
    "\n",
    "acc_grad = th.autograd.functional.jacobian(l, y) * acc_grad\n",
    "\n",
    "W_j, b_j = th.autograd.functional.jacobian(f, (W, b))\n",
    "print(W.grad.equal(W_j.permute(0, 2, 1) @ acc_grad))\n",
    "print(b.grad.equal(b_j.t() @ acc_grad))\n",
    "\n",
    "# or\n",
    "acc_grad = e.grad\n",
    "print(acc_grad)\n",
    "grad_L_y = th.autograd.functional.jacobian(l, y)\n",
    "acc_grad = grad_L_y * acc_grad\n",
    "print(acc_grad)\n",
    "grad_y_w, grad_y_b = th.autograd.functional.jacobian(f, (W, b))\n",
    "acc_grad = grad_y_w.permute(0, 2, 1) @ acc_grad\n",
    "print(acc_grad.equal(W.grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8150aa2b",
   "metadata": {},
   "source": [
    "### Custom Autograd Functions \n",
    "Fine grained control over gradients and computations <br>\n",
    "$ y = 3x^2 + 2x + 1$ <br>\n",
    "$ dy \\over dx \\\\= 6x+2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fa57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticFunction(th.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 3*input**2 + 2*input + 1\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 6*input + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6972d04-7bda-4e37-9d15-6d3d63e6f118",
   "metadata": {},
   "source": [
    "![relu.png](../Presentations/assets_autograd/relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91938be-9426-4b05-a008-c4bca8b8d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLUFunction(th.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # compute the result of forward pass and save input for backward pass\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # grad_output = gradient of the loss (accumulated gradient) w.r.t. output\n",
    "        # we need to compute the gradient of the loss w.r.t. input\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        grad_input[input >= 0] = 1\n",
    "        return grad_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "537069c9",
   "metadata": {},
   "source": [
    "### Turning on/off\n",
    "Multiple options:\n",
    "* Without copy: $$ x.requires\\_grad=True$$\n",
    "* With copy:  $$ x.detach() $$\n",
    "* Turning it off(on) temporarely using context manager: $$ with\\ torch.no\\_grad() \\ \\\\or\\ with\\ torch.enable\\_grad() $$\n",
    "* For a group of operations using decorator: $$ @torch.no\\_grad() \\ \\\\ or\\ @torch.enable\\_grad() $$\n",
    "\n",
    "Use case: Freezing some layers: <br>\n",
    "![freeze.png](../Presentations/assets_autograd/freeze.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f777995-e745-4b5c-9fd6-36c1231e238a",
   "metadata": {},
   "source": [
    "Without copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c1b79-1db2-43d4-9c00-d90f6da71c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = th.ones(10, requires_grad=True)\n",
    "w2 = th.ones(10, requires_grad=True)\n",
    "\n",
    "y = w1*2 + w2*3\n",
    "\n",
    "# set this before !!\n",
    "w1.requires_grad = False\n",
    "w2.requires_grad = False\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234da17-9e8f-4e50-888f-c909d5c0888e",
   "metadata": {},
   "source": [
    "Temporarely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd470b-29d7-469e-8538-1c2816c54e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = th.ones(10, requires_grad=True)\n",
    "w2 = th.ones(10, requires_grad=True)\n",
    "\n",
    "with th.no_grad():\n",
    "    y2 = w1*2 + w2*3\n",
    "    print(y2)\n",
    "\n",
    "y3 = w1*2 + w2*3\n",
    "print(y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef2b68-8d19-465c-ab79-9c4684b4943c",
   "metadata": {},
   "source": [
    "Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bda83-418a-4a6b-8008-9d6c5d8de665",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = th.ones(10, requires_grad=True)\n",
    "w2 = th.ones(10, requires_grad=True)\n",
    "             \n",
    "@th.no_grad\n",
    "def linear_func(w1, w2):\n",
    "    return w1*2 + w2*3\n",
    "    \n",
    "y4 = linear_func(w1, w2)\n",
    "print(y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effaee68-21ec-4749-ba1d-694b9987ffca",
   "metadata": {},
   "source": [
    "With copy <br>\n",
    "Use case: perform intermediate computations without affecting the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44691b5-9d6e-4ab6-a7e0-80d0434a9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = th.ones(10, requires_grad=True)\n",
    "w2 = th.ones(10, requires_grad=True)\n",
    "\n",
    "y5 = w1.detach()*2 + w2*3\n",
    "make_dot(y5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217087f4",
   "metadata": {},
   "source": [
    "### Gradient reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b696650",
   "metadata": {},
   "source": [
    "Reset gradients as they get accumulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = th.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# make it smaller\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(588, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = th.flatten(x, 1) \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(zero_gradients=True):\n",
    "    net = Net()\n",
    "    net.to('cuda')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(5):  \n",
    "        \n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "\n",
    "            if zero_gradients:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_history.append(loss.item())\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "zg = train(zero_gradients=True)\n",
    "nzg = train(zero_gradients=False)\n",
    "\n",
    "plt.plot(zg)\n",
    "plt.plot(nzg)\n",
    "plt.legend(['Zero gradients', 'No zero gradients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f94b4",
   "metadata": {},
   "source": [
    "### Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca4cd3-8281-425e-b0aa-a1ea536e052e",
   "metadata": {},
   "source": [
    "Track CPU+GPU times for each function (executed in C++) and its corresponding derivative function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cpu')\n",
    "run_on_gpu = False\n",
    "if th.cuda.is_available():\n",
    "    device = th.device('cuda')\n",
    "    run_on_gpu = True\n",
    "    \n",
    "B, Cin, Ns = 32, 16, 100    \n",
    "x = th.randn(B, Cin, Ns)\n",
    "conv1d = th.nn.Conv1d(16, 32, kernel_size=3)\n",
    "\n",
    "with th.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        y = conv1d(x)\n",
    "        \n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322bfc34",
   "metadata": {},
   "source": [
    "Resources:\n",
    "* https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec\n",
    "* https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#what-do-we-need-autograd-for\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://pytorch.org/docs/stable/notes/extending.html\n",
    "* https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197051fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calib_transf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
