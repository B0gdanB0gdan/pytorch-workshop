{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d8a08d-6c33-40e2-afe4-f5f6d555810c",
   "metadata": {},
   "source": [
    "# Goals\n",
    "Understand how PyTorch implements the concept of \"tensor\" in terms of:\n",
    "* properties (storage, dimension, data types, tensor types, structure)\n",
    "* expressivity \n",
    "* bridge with NumPy arrays\n",
    "* most common operations\n",
    "* usage hints \n",
    "* practical examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988b08c-4169-43b3-9926-186f783e373b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d609c7c-fdb2-45f9-955b-4d0538ee1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4974a6-cf21-4629-a6a9-dc6f1fe2f358",
   "metadata": {},
   "source": [
    "# 1. Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627959a-0ed2-4aa2-bd09-8950edc524c3",
   "metadata": {},
   "source": [
    "### 1.1. Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84afb3a-5958-4b42-ab4f-2b9ebfa64e41",
   "metadata": {},
   "source": [
    "Q: What is the inner structure of the th.Tensor class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8b6f6-bb6a-4e39-8431-5982c4204430",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [el for el in dir(th.tensor(0)) if \"_\" not in el]\n",
    "print(props, len(props))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b807d-70ae-45d1-a12b-f3eb0eee89f0",
   "metadata": {},
   "source": [
    "Q: What are the most important characteristics ? <br>\n",
    "Let's take a look at: \n",
    "* layout\n",
    "* device\n",
    "* dtype\n",
    "* size\n",
    "* shape\n",
    "* storage\n",
    "* nbytes\n",
    "* strides\n",
    "* itemsize\n",
    "* ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a8525-8459-414e-8e62-6e5a89699050",
   "metadata": {},
   "source": [
    "![contiguous.png](../Presentations/assets/contiguous.png)\n",
    "![non_contiguous.png](../Presentations/assets/non_contiguous.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3ed58-169d-4cc3-90f1-ec1851b0364a",
   "metadata": {},
   "source": [
    "stride != 1 --> not contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab15b8-9f46-4056-abb8-5ab4bf42dbf1",
   "metadata": {},
   "source": [
    "The following fields are related to the conceptual tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f2ff5-9973-45bd-b3ee-6de786a7dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.rand((2, 2))\n",
    "\n",
    "print(\n",
    "    x.data, \n",
    "    x.dtype,\n",
    "    x.size(), # alias for .shape\n",
    "    x.ndim,\n",
    "    x.device,\n",
    "    x.stride(), # how data is arranged in memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e9a3c-ac8d-446c-8e38-cc403adcce48",
   "metadata": {},
   "source": [
    "The following fields are related to the physical tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3449061f-e4b2-4624-a474-f92633f19ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 2**31-1], dtype=th.int32)\n",
    "\n",
    "print(x.nbytes, x.itemsize, x.layout) # strided = dense or sparse\n",
    "print(x.untyped_storage()) # untyped array of bytes, view acts on this\n",
    "print(x.untyped_storage().data_ptr()) # address of first element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb69380-d535-448f-adc6-ce90277a864e",
   "metadata": {},
   "source": [
    "Q: Does PyTorch infer the data type automatically? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d0a05-704c-4fc5-8cf7-3745b7bebe18",
   "metadata": {},
   "source": [
    "Default type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21b50e-c3b4-46e6-91ac-b2dc25cc6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.Tensor(1) # uses a global default type and provides a way to define empty tensors\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c78448-13a1-41b6-9b7b-8cde12812eb6",
   "metadata": {},
   "source": [
    "Type inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611f407-b9d6-4a55-90a9-7b0165be5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 3]) # automatically infered\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d7001-d196-406f-97be-2625f901d0ff",
   "metadata": {},
   "source": [
    "Type borrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec643772-8593-4066-a5d6-24a61193967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array([1, 2, 3], dtype=np.int16)\n",
    "x_th = th.from_numpy(x_np)\n",
    "x_th.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1af8b-77f0-4404-9824-26e881cc24ce",
   "metadata": {},
   "source": [
    "Type promotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46d1b2-3520-4f68-a9c8-fa9dccbe0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor(1, dtype=th.int16)\n",
    "y = th.tensor(1, dtype=th.int32)\n",
    "(x+y).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fa188-cd23-47b1-ac9f-5c3e1688c2ab",
   "metadata": {},
   "source": [
    "Type overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d6775-6e24-48e2-8afd-d28f9abe573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_int = 2**31-1\n",
    "x = th.tensor(max_int, dtype=th.int32)\n",
    "y = th.tensor(max_int + 1)\n",
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47600cd-8ce0-4e77-b2e5-1d6cc3d542d0",
   "metadata": {},
   "source": [
    "### 1.2. Working with dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5422c81-5fd8-49ea-936d-3bcd59eb0420",
   "metadata": {},
   "source": [
    "![Dimensions.png](../Presentations/assets/dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c1fc5-6027-4235-88a7-611aeb6ca478",
   "metadata": {},
   "source": [
    "Q: How operations apply on dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e110432-fb39-433e-ab91-193bc8fa7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([\n",
    "               [[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]],\n",
    "               \n",
    "               [[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]]\n",
    "              ])\n",
    "th.sum(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2af20a-184d-4a48-9b3b-423d2c66ddc9",
   "metadata": {},
   "source": [
    "Q: What memory format does it use and how the choice can impact performance? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c6c6d-1ccd-4bab-bf5e-76759dda9b48",
   "metadata": {},
   "source": [
    "![memory_layout.png](../Presentations/assets/memory_layout.png) <br>\n",
    "\n",
    "In PyTorch the default memory format is <b>Channel First</b> <br>\n",
    "Usually we think about it for vision models (B, C, H, W) as Channel Last format is implemented for 4D Tensors only -> e.g., conv, batch_norm <br>\n",
    "It depends on the used backend <br>\n",
    "Performance gains can be achieved using channel last on MKL-DNN (Intel Xeon >= Ice Lake (26-76%), Volta devices with cuDNN > 7.6 (22%)) <br>\n",
    "In case a particular operator doesn't support Channel Last, the NHWC input will be treated as non-contiguous NCHW <br>\n",
    "General rule of memory format propagation: <br>\n",
    "* Channel first input -> Channer first output <br>\n",
    "* Channel last input -> Channer last output <br>\n",
    "* If operation not supported -> permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae657390-26e4-4d3a-9ade-e39b9ee2da7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N, C, H, W = 10, 3, 32, 32\n",
    "x = th.empty(N, C, H, W)\n",
    "print(x.stride(), x.is_contiguous())  \n",
    "# HWC x 1 x WC x C\n",
    "x = x.to(memory_format=th.channels_last)\n",
    "print(x.stride(), x.is_contiguous())\n",
    "# for-loops durations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9e5e9-210e-4075-a981-10cf4ff06fd8",
   "metadata": {},
   "source": [
    "Q: How can we use a different memory format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff64753-7b4e-4bf3-b10e-577951416667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "N, C, H, W = 1, 3, 224, 224\n",
    "x = th.rand(N, C, H, W)\n",
    "model = resnet50()\n",
    "model.eval()\n",
    "\n",
    "# convert input and model to channels last\n",
    "x = x.to(memory_format=th.channels_last) \n",
    "model = model.to(memory_format=th.channels_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7186c025-6b51-4932-bd7d-a6a535c581e1",
   "metadata": {},
   "source": [
    "Scope of <b>Channel Last</b> support:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86ef11-af27-4efe-89a9-6783b51fcc1e",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533bbb2-0a72-437e-8aa7-6df86a4f5d9b",
   "metadata": {},
   "source": [
    "# 2. Expressivity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97447cc9-c125-4799-9463-7534e8694f26",
   "metadata": {},
   "source": [
    "Q: Homogeneous or Heterogeneous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9df090-77bf-4e31-8e5d-232beabb9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 3], dtype=th.int32)\n",
    "# x[0] = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0d730-e03c-4c1b-83b3-df2463923907",
   "metadata": {},
   "source": [
    "Q: What data can we represent with tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c3b98-6204-4868-a4c8-3089a6c599e5",
   "metadata": {},
   "source": [
    "Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b8802-76de-4e11-8a35-8b83d5aeef6b",
   "metadata": {},
   "source": [
    "![vectors.png](../Presentations/assets/vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005ea4c-a433-4a4e-91a9-1b1f0338d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = th.tensor([1, 0])\n",
    "v = th.tensor([0, 1])\n",
    "w = u + v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc281a7-0bb0-4f23-96b6-00cc7b2ab6f5",
   "metadata": {},
   "source": [
    "Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b432df5-5ba4-48d0-b5e3-40ffce9dd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv('../Presentations/assets/iris.csv').iloc[:, :-1].to_numpy()\n",
    "data = th.from_numpy(iris_data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad2cc5-fe45-4576-9aed-e1a27f597e0b",
   "metadata": {},
   "source": [
    "Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40e2f5-98dd-4d17-8a1c-cc8d6c91113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"Why did the AI cross the road? To optimize its algorithm for chicken recognition!\".split(' ')\n",
    "word2index = {k:i for i, k in enumerate(seq)}\n",
    "emb = th.nn.Embedding(32, 3)\n",
    "th_ind = th.tensor(list(word2index.values()), dtype=th.int)\n",
    "emb(th_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dbda5-e0ab-4bf3-8867-107ba3ccf823",
   "metadata": {},
   "source": [
    "Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5eb874-fa92-4d6d-8d5e-ef91bb2b2c83",
   "metadata": {},
   "source": [
    "![graph.png](../Presentations/assets/graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26579a98-9df6-41ce-ac50-aaf6e9e49fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = th.tensor([1, 2, 3, 4])\n",
    "adj_matrix = th.tensor([[0, 1, 0, 0], [1, 0, 0, 1], [0, 0, 0, 1], [0, 1, 1, 0]])\n",
    "# check PyTorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33989460-a657-4782-afce-f0a4f0b02388",
   "metadata": {},
   "source": [
    "Nesting - native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb6e0c-161f-4aa9-9bf3-7a269a5cc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = th.tensor([1, 2])\n",
    "line2 = th.tensor([3, 4])\n",
    "# matrix = th.tensor([line1, line2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7d380-ce56-42a9-b4fd-fb719014f868",
   "metadata": {},
   "source": [
    "Nesting - specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478dec5-425e-40de-a31c-91789a925a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = th.nested.nested_tensor([line1, line2], dtype=th.float32)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d01a2-07f1-4c0e-877b-66baf8f838fe",
   "metadata": {},
   "source": [
    "N-dimensional data (N>=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2957d-896b-4bb8-91e5-3fefc9889e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision handles images and videos\n",
    "image = Image.open(\"../Presentations/assets/cayenne.png\")\n",
    "plt.imshow(image)\n",
    "image_th = th.from_numpy(np.array(image))\n",
    "image_th.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f8e49-ad3c-490c-a421-306cde5ebbe1",
   "metadata": {},
   "source": [
    "# 3. Bridge with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d29dff-151e-45c1-8c7f-f40713170f31",
   "metadata": {},
   "source": [
    "Q: What are the similarities and differences between pt and np?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd3094-799d-4a9f-abd8-212694d1a26c",
   "metadata": {},
   "source": [
    "pt <-> np conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6cf1c-f7c7-4b9d-a9a6-cb1ff5be964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array(1)\n",
    "x2 = th.tensor(1)\n",
    "y1 = th.from_numpy(x1)\n",
    "y2 = x2.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3908143-59e0-4418-b72b-af66aeef6f81",
   "metadata": {},
   "source": [
    "Changes are reflected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9351c-b496-4f36-99ea-2027db1a6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.ones(3,)\n",
    "x_th = th.from_numpy(x_np)\n",
    "x_np[0] = 2\n",
    "print(x_th)\n",
    "x_th[0] = 3\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cdf49-a6de-4d4e-b97b-6f0a559462fc",
   "metadata": {},
   "source": [
    "as_tensor vs tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ff08a-1da6-4dbc-a5cf-5b575056dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array([1, 2, 3])\n",
    "x_th1 = th.tensor(x_np)\n",
    "x_th2 = th.as_tensor(x_np)\n",
    "x_th3 = th.from_numpy(x_np)\n",
    "\n",
    "print(x_th1.untyped_storage().data_ptr() == x_th2.untyped_storage().data_ptr())\n",
    "print(x_th2.untyped_storage().data_ptr() == x_th3.untyped_storage().data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c0c6e-df22-4653-b319-e39ce293f4e5",
   "metadata": {},
   "source": [
    "# 4. Most common operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11388894-4fcf-4389-8e03-24fcc6b86299",
   "metadata": {},
   "source": [
    "### 4.1. Declaration\n",
    "What are the factory methods?\n",
    "Let's explore different options:\n",
    "* empty\n",
    "* randomly\n",
    "* zeros/ones\n",
    "* full(of)\n",
    "* from python/numpy/another tensor\n",
    "* linspace/ranges\n",
    "* to\n",
    "* float/int/short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da19f5-1857-47ad-b36c-7f1f9aef1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.empty((2,3), dtype=th.int64)\n",
    "th.randn((2, 2))\n",
    "th.zeros((2, 2, 2))\n",
    "3*th.ones(3,)\n",
    "th.full((2, 3), np.pi)\n",
    "th.tensor([1, 2, 3])\n",
    "x = th.from_numpy(np.array([1, 2, 3]))\n",
    "y = th.ones_like(x)\n",
    "th.linspace(0, 1, 100)\n",
    "th.arange(5, 20, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c148af5-8113-4efd-8d40-958f893888a2",
   "metadata": {},
   "source": [
    "### 4.2. Indexing & Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a83a7a-410e-494a-ab0f-0927887cccbc",
   "metadata": {},
   "source": [
    "Q: How expressive is slicing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803012a-59f5-4a07-a0ba-260eef909c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn((5, 5, 5, 5, 5))\n",
    "y = x[:, 1:3, -1, 2:, [0, 2, 4]]\n",
    "x[0, 0, 0, 0, 1:4] = th.zeros(3,)\n",
    "x[..., 1:4] = th.zeros(3,)\n",
    "y1 = x[..., ::2]\n",
    "print(y1.is_contiguous())\n",
    "y2 = x[..., [0, 2, 4]]\n",
    "print(th.equal(y1, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041080c3-59ab-441f-9ca3-a2eb681822f4",
   "metadata": {},
   "source": [
    "### 4.3. Shape manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca61cd-3472-4f2c-a2e9-52affc2a0846",
   "metadata": {},
   "source": [
    "Q: How PyTorch handles broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef6309-2f24-4567-a20b-64abeaef177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.empty(5,1,4,1)\n",
    "y = th.empty(  3,1,1)\n",
    "print((x+y).size())\n",
    "\n",
    "x = th.empty(5,1,4,1)\n",
    "y = th.empty(  3,2,1)\n",
    "(x+y).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff1b66-8a64-4826-9901-574045b463c8",
   "metadata": {},
   "source": [
    "Q: I know global average pooling layers are better but still how do I flatten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcdce5-2121-440d-9e51-7b10c3228174",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([[1, 2, 3],\n",
    "               [4, 5, 6],\n",
    "               [7, 8, 9]])\n",
    "\n",
    "th.flatten(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70985c2-80dc-4568-b34f-75a12d4fbd6c",
   "metadata": {},
   "source": [
    "Q: I often need to add a dimension (e.g., when running inference on a single example), how do I do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d94e9-803d-4737-86a8-ddfa7382153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.zeros((3,3))\n",
    "y = x.unsqueeze(0).unsqueeze(-1)\n",
    "print(x.size(), y.size())\n",
    "print(y.squeeze().size())\n",
    "print(x[None, ...].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b7a3c-93af-4fb5-b0ba-44a55c121dc1",
   "metadata": {},
   "source": [
    "### 4.4. Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a950e8-e32d-4fe0-b5b2-71b910043ae8",
   "metadata": {},
   "source": [
    "Q: What are views and what can I do with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa7e51-6c1d-40ed-8f97-e7fda4198d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn((4,4))\n",
    "y = x.reshape((8, 2))\n",
    "z = x.view((8, -1))\n",
    "print(\n",
    "    y.untyped_storage().data_ptr() == z.untyped_storage().data_ptr(),\n",
    "    y.is_contiguous(),\n",
    "    z.is_contiguous()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68949441-a8fb-4529-99b4-788f7e4b0e1b",
   "metadata": {},
   "source": [
    "Q: Are views contiguous?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b998361-6ba6-4865-a43e-22ce397fc800",
   "metadata": {},
   "source": [
    "![views.png](../Presentations/assets/views.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab21b1d-5439-4e2f-aa1d-ecb0ed00c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([[1, 2], [3, 4]])\n",
    "print(x[:, 1].size(), x[:, 1].stride(), x[:, 1].storage_offset(), x[:, 1].is_contiguous())\n",
    "print(x[1, :].size(), x[1, :].stride(), x[1, :].storage_offset(), x[1, :].is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c7115-66f7-46af-b1bb-5161b87a7079",
   "metadata": {},
   "source": [
    "List of operations producing views (however some of them may produce a tensor):\n",
    "https://pytorch.org/docs/stable/tensor_view.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51daa57a-cd25-44a1-97ee-640b959e6522",
   "metadata": {},
   "source": [
    "### 4.5. Joining vs Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffc38e-d4fd-484a-9c71-47f4ec45765b",
   "metadata": {},
   "source": [
    "Q: How can I stack or partition a set of elements? <br>\n",
    "Q: I see two methods of doing that, what is the difference between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf0c69-7f45-441b-abe2-0c283c74a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = th.randn((3, 4))\n",
    "x2 = th.randn((3, 4))\n",
    "x3 = th.randn((3, 4))\n",
    "y1 = th.cat([x1, x2, x3], dim=0)\n",
    "y2 = th.stack([x1, x2, x3], dim=0)\n",
    "y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4832c-a1e9-4e43-8b62-0d0fce3b86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([[-3, -2, -1],\n",
    "                  [0, 1, 2],\n",
    "                  [3, 4, 5],\n",
    "                  [6, 7, 8]])\n",
    "\n",
    "print(th.unbind(x))\n",
    "print(th.unbind(x)[0].size())\n",
    "print(th.split(x, split_size_or_sections=2))\n",
    "print(th.split(x, split_size_or_sections=2)[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1937aa-bad0-48a1-a5c3-ca881dfb0026",
   "metadata": {},
   "source": [
    "### 4.6. Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ca6af-a5f5-4fd5-a435-65c94efe41cc",
   "metadata": {},
   "source": [
    "Q: Matlab has this, what about you PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9811d-a1d4-4328-ab20-a7e9425b2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = th.tensor([[1, 2], [3, 4]], dtype=th.float32)\n",
    "B = th.tensor([[5, 6], [7, 8]], dtype=th.float32)\n",
    "A+B, A-B, A*B, A/B\n",
    "print(th.equal(th.transpose(A, 0, 1), A.T))\n",
    "Ai = th.inverse(A)\n",
    "Ad = th.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00698b-f8ab-4184-a37f-ce9e7864fc01",
   "metadata": {},
   "source": [
    "More common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e28d9-fd40-4a3b-a662-f94c70079245",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(th.eye(3))\n",
    "v = th.randn(3, 3)\n",
    "print(th.diag(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a2e6034-b991-401b-b033-a5d25311deb1",
   "metadata": {},
   "source": [
    "### 4.7. BLAS & LAPACK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef670bc5-48ea-4e5c-8547-fbcf62ee27a8",
   "metadata": {},
   "source": [
    "Q: How do i solve a linear system of equations? <br>\n",
    "\\begin{alignedat}{5}\n",
    "     x& {}+{} & 2y& = &2 \\\\\n",
    "     3x& {}+{} & 4y& = &3\n",
    "\\end{alignedat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78253e-79d5-4e1c-b4d0-f85a6d35fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = th.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = th.tensor([2.0, 3.0])\n",
    "\n",
    "x = th.linalg.solve(A, b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc91d1e-c5a4-468a-a4eb-185c10335a5c",
   "metadata": {},
   "source": [
    "Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ef24f-41c9-4021-a7b3-60fa5a356fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = th.tensor([[1, 2], [3, 4]], dtype=th.float32)\n",
    "B = th.tensor([[5, 6], [7, 8]], dtype=th.float32)\n",
    "print(th.equal(th.mm(A, B), A@B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d3d58-7a29-4066-985a-f6ad2994c067",
   "metadata": {},
   "source": [
    "Q: I need to implement PCA on my own, how do you get eigen values and vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201ea64-0310-415b-b8e4-1aed6cf1e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = th.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "eigenvalues, eigenvectors = th.linalg.eig(A)\n",
    "print(eigenvalues)\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcae9e-2ba7-4479-9873-f5403de73525",
   "metadata": {},
   "source": [
    "Q: Attention is all you need ... but do you need \"scaled dot product attention\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c99464-134e-412d-9753-6dd684a67d53",
   "metadata": {},
   "source": [
    "![attention.png](../Presentations/assets/attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e325b-d274-4e17-9f11-cb26943af707",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE, NUM_KEYS, FEAT_SIZE = 32, 10, 3\n",
    "keys = th.randn(BATCH_SIZE, NUM_KEYS, FEAT_SIZE)\n",
    "query = th.randn(BATCH_SIZE, 1, FEAT_SIZE)\n",
    "\n",
    "scores = th.bmm(query, keys.transpose(1, 2))  # (B, 1, F) b@ (B, F, N) => (B, 1, N)\n",
    "scores /= th.sqrt(th.tensor(FEAT_SIZE, dtype=th.float32))\n",
    "\n",
    "att_weights = th.nn.functional.softmax(scores, dim=-1)\n",
    "att_vec = th.bmm(att_weights, keys)  # (B, 1, N) b@ (B, N, F) => (B, 1, F)\n",
    "print(att_weights.size(), att_vec.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90cbb31-c167-4d5a-b5e1-622ed6aa820c",
   "metadata": {},
   "source": [
    "### 4.8. In-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf482e84-f29d-4691-b964-e56a060f8112",
   "metadata": {},
   "source": [
    "Q: How can I avoid creating new tensors for my operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec67d2-d61c-412e-86b5-2dc847881388",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor(1)\n",
    "y = th.tensor(3)\n",
    "x.add_(1)\n",
    "print(x.item())\n",
    "x.copy_(y)\n",
    "print(y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca50873-2510-4bc2-9a1e-c09076efe44a",
   "metadata": {},
   "source": [
    "Q: Python has \"map\" to traverse a list and apply a function on each element, what does PyTorch have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31274597-59df-44a0-9bae-e6608fa0bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_list = [th.randn(4,4), th.randn(3,3), th.randn(2,2)]\n",
    "th._foreach_abs(torch_list)\n",
    "new_list = th._foreach_sigmoid(torch_list)\n",
    "print(new_list[0] is torch_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8eb68-805a-4d1f-8bff-b6d36b6991a4",
   "metadata": {},
   "source": [
    "### 4.9. Cloning operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c83c9-0493-476d-b762-cb12b072f1a0",
   "metadata": {},
   "source": [
    "Q: Can i do both shallow and deep copies?\n",
    "* view() vs clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17de06e-a420-4345-bedc-b5ebffa0e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 3])\n",
    "x_clone = x.clone()\n",
    "x_clone[0] = 5\n",
    "print(x, x_clone)\n",
    "x_view = x.view(-1)\n",
    "x_view[0] = 5\n",
    "print(x, x_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca7155-dc38-4520-a0da-806ec260e281",
   "metadata": {},
   "source": [
    "Q: Clone -> Detach or Detach -> Clone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97fb6d1-19f7-4f8e-81c1-fd185fc33b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = th.tensor(1.0, requires_grad=True)\n",
    "print(w.untyped_storage().data_ptr())\n",
    "w1 = w.clone()\n",
    "print(w1.untyped_storage().data_ptr(), w1.grad_fn)\n",
    "w2 = w.detach()\n",
    "print(w2.untyped_storage().data_ptr(), w2.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c8fa2-741f-4be8-923b-ab7936607faf",
   "metadata": {},
   "source": [
    "### 4.10. Random number generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e078e9-072e-4e69-bc21-3c8fed59af9b",
   "metadata": {},
   "source": [
    "Q: How can I make my experiments deterministic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5c4ce-447a-4e8d-8f3e-40092b5acba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d29a31-b7d9-4202-b057-61888b9f783c",
   "metadata": {},
   "source": [
    "![categorical.png](../Presentations/assets/categorical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16dbc90-bbd9-4f0e-9cc1-17e2c66c828a",
   "metadata": {},
   "source": [
    "Q: How can I train an RL agent that has 5 actions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee0763-d100-48b1-a2fe-4bf7bf319857",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = th.randn(32,)\n",
    "policy_network = th.nn.Sequential(\n",
    "    th.nn.Linear(32, 5),\n",
    "    th.nn.Softmax()\n",
    ")\n",
    "probs = policy_network(state)\n",
    "m = th.distributions.Categorical(probs=probs)\n",
    "action = m.sample()\n",
    "\n",
    "class Env:\n",
    "    def step(self, action):\n",
    "        return th.randn(32,), th.randn(1,)\n",
    "env = Env()\n",
    "next_state, reward = env.step(action)\n",
    "loss = -m.log_prob(action) * reward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d888221-b91b-476f-ad99-3c1b9ccb5564",
   "metadata": {},
   "source": [
    "### 4.11. Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20153ef-bfaa-432b-ad7a-f1c7caca1d57",
   "metadata": {},
   "source": [
    "Q: I want to resume training later, how can I do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc915a-77f3-45c7-8179-0c9b6d687a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 3])\n",
    "th.save(x, \"example_tensor.pt\")\n",
    "th.load(\"example_tensor.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235197c5-52ad-4edc-8cf1-eb1e212c1d17",
   "metadata": {},
   "source": [
    "Q: I saved a very small tensor but the actual size on disk is much larger. Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf46d0-5b20-4123-9bf7-478ee7839357",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.arange(0,10)\n",
    "y = x[:5]\n",
    "th.save([x, y], \"example_tensor_list.pt\")\n",
    "x, y = th.load(\"example_tensor_list.pt\")\n",
    "y -= 1 \n",
    "print(x)\n",
    "th.save(y, \"example_view.pt\")\n",
    "y = th.load(\"example_view.pt\")\n",
    "print(y.storage().size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c28158-57ed-449c-be4d-427b7a0afcce",
   "metadata": {},
   "source": [
    "Q: What is actually saved/serialized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518fafbe-9e13-41e9-890b-2a4654038b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_module = th.nn.BatchNorm1d(1)\n",
    "th.save(ex_module.state_dict(), 'batch_norm.pt')\n",
    "bn_state_dict = th.load('batch_norm.pt')\n",
    "for k, v in bn_state_dict.items():\n",
    "    print(k, v)\n",
    "ex_module = th.nn.BatchNorm1d(1)\n",
    "ex_module.load_state_dict(bn_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1e831-2357-42c9-ba14-2c0c956b3713",
   "metadata": {},
   "source": [
    "### 4.12. Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157fdd86-2b3e-4cc6-aa36-b962a3a036e7",
   "metadata": {},
   "source": [
    "Q: How can I perform frequency analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bcfb39-4024-4d2c-9567-68990bb998c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = th.randn(1000)\n",
    "plt.plot(waveform)\n",
    "plt.show()\n",
    "spec = th.stft(waveform, n_fft=128, hop_length=16, return_complex=True)\n",
    "mag_spec = th.abs(spec)\n",
    "phase_spec = th.angle(spec)\n",
    "plt.imshow(th.log10(mag_spec + 1e-6))\n",
    "plt.show()\n",
    "complex_spec = mag_spec * th.exp(1j * phase_spec)\n",
    "waveform_reconstructed = th.istft(complex_spec, n_fft=128, hop_length=16, length=1000)\n",
    "print(th.pow((waveform - waveform_reconstructed), 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab10d20-65b9-47b4-ac38-c4d60003261d",
   "metadata": {},
   "source": [
    "### 4.13. Reduction operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd0a9b-ac29-46d8-bb9a-d1fa4765f65f",
   "metadata": {},
   "source": [
    "Q: How do I classify this image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea751a6-4a71-4bfa-a1df-08d736c463f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 3])\n",
    "print(x.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0f2ca-cdac-4801-97ff-da4b064d5706",
   "metadata": {},
   "source": [
    "Q: My segmenation label is entirely black, how can I discard it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed3ce3-6b40-4e28-b18d-75e50ed71618",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = th.tensor([False, False, True])\n",
    "print(y.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e4a64-07b8-40ee-a279-d7d606ea0233",
   "metadata": {},
   "source": [
    "### 4.14. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6efb9e-cdff-4a57-b7d7-90c27d0f3ed1",
   "metadata": {},
   "source": [
    "Q: How can I compare two tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac318a-2be9-40fd-8d9f-fe4b6a4a1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = th.tensor([1, 2, 3])\n",
    "x2 = th.tensor([1, 2, 3])\n",
    "print(th.equal(x1, x2))\n",
    "print(th.eq(x1, x2))\n",
    "print(x1 == x2)\n",
    "print(x1 is x2)\n",
    "print(x1 < x2)\n",
    "print(x1.isnan())\n",
    "print(x1.topk(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c78d0-ef44-4c39-939a-0935bf1387c9",
   "metadata": {},
   "source": [
    "### 4.15. Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a3ba8-0663-47c6-b2af-9c937ab533f2",
   "metadata": {},
   "source": [
    "Q: I have some padded image but I don't want my loss to account for that, how can I ignore padding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb07817-ca5e-4e79-a423-cfe37e7eda5a",
   "metadata": {},
   "source": [
    "![branch.png](../Presentations/assets/branch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680b808-ea95-4885-a2ac-8427be012807",
   "metadata": {},
   "source": [
    "![masked_tensor.png](../Presentations/assets/masked_tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270a4ef-2a50-4803-86b1-54ac2df4d2f0",
   "metadata": {},
   "source": [
    "Conditional selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fae8f-edea-4368-8312-044bf81ada86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 3, 4, 5])\n",
    "print(x[x<3])\n",
    "print(x & 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710f487-d7cd-41c3-b662-d4ac635f8a93",
   "metadata": {},
   "source": [
    "Inline branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0ce3c-3b46-4e52-9076-5087063de879",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1, 2, 3, 4, 5])\n",
    "th.where(x>3, th.tensor(1), th.tensor(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6eac20-8f07-4b17-9f0f-96cb3ea94ea0",
   "metadata": {},
   "source": [
    "Q: My data is sparse, how can I store it efficiently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc140a-2da2-47a8-be97-b539491987b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = th.tensor([[0, -1, -2],\n",
    "                     [0,  0,  0],\n",
    "                     [-3, 0, -4]])\n",
    "mask = (data != 0)\n",
    "masked_data = th.masked.masked_tensor(data, mask)\n",
    "print(data.amax(), masked_data.amax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26086d6-8228-4e24-9d6a-c7ae20aa87fa",
   "metadata": {},
   "source": [
    "### 4.18. Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109589c-931c-4b46-82fa-b72d840c703d",
   "metadata": {},
   "source": [
    "Q: Found this paper where an agent navigates with lidars placed around him, how can I process that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a157a6a-1ce6-47a7-a5f9-ebde4818a1b3",
   "metadata": {},
   "source": [
    "![openai.jpg](../Presentations/assets/openai.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40c1d8-a567-44bc-a82e-6d068943941e",
   "metadata": {},
   "source": [
    "padding types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04c457-e47f-4fc6-aeb8-7ffa6ba62d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = th.ones(3, 2)\n",
    "padding_size = (2, 2)  # (left, right, top, bottom)\n",
    "th.nn.functional.pad(input, padding_size, mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1115d4b-70b2-4eec-bce1-f8fc0b9b1ad7",
   "metadata": {},
   "source": [
    "Circular convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec77a95-4cdc-4f83-bbde-ab6f10086ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_signal = th.randn(1, 1, 8)\n",
    "kernel_size = 3\n",
    "padding_size = (kernel_size-1, kernel_size-1)\n",
    "padded_signal = th.nn.functional.pad(lidar_signal, padding_size, mode='circular')\n",
    "conv_layer = th.nn.Conv1d(in_channels=1, \n",
    "                            out_channels=1, \n",
    "                            kernel_size=kernel_size)\n",
    "circ_conv_res = conv_layer(padded_signal)\n",
    "print(circ_conv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b13411-f17a-48dd-add4-bd9daf61b5f5",
   "metadata": {},
   "source": [
    "### 4.19. CPU-GPU Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394bb33-1bcb-4b2f-8e94-e1344a77c981",
   "metadata": {},
   "source": [
    "Q: How can I increase performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03aac3-a941-42ef-a3f4-be8e7add9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_tensor = th.Tensor([1, 2, 3])\n",
    "gpu_tensor1 = cpu_tensor.cuda()\n",
    "gpu_tensor2 = cpu_tensor.to(device=\"cuda\")\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "gpu_tensor3 = cpu_tensor.to(device)\n",
    "cpu_tensor = gpu_tensor3.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cad1fa-c506-4c9d-bfe2-adfef8b0f174",
   "metadata": {},
   "source": [
    "Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2fd6e-b13c-4e6f-857c-86a701749494",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = th.device('cuda')     # Default \n",
    "cuda2 = th.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796c183-9008-4698-8932-b6d4e15455e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calib_transf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
