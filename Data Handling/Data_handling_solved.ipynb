{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c255a8-be19-4c86-8852-a5e9efc1bf79",
   "metadata": {},
   "source": [
    "### Goal\n",
    "* how to decouple the training code from the dataset code\n",
    "* how to process large amounts of data\n",
    "* understand Pytorch particularities\n",
    "* end-to-end example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff6c6d-4326-497e-8164-cca75e607279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e47c3b-8bb7-4e28-85bf-50aec7d0378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torchvision.datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65267e6c-2a97-482b-a66f-723c60ee183d",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f2876-d1e1-4c02-ab08-1476077154ae",
   "metadata": {},
   "source": [
    "Two data primitives to achieve decoupling: \n",
    "* <b>torch.utils.data.Dataset</b> as a data store for (sample, label) pairs\n",
    "* <b>torch.utils.data.DataLoader</b> wraps an iterator over the data store\n",
    "\n",
    "Different built-in datasets that subclass <b>torch.utils.data.Dataset</b>:\n",
    "* <b>Image datasets</b>: classification, object detection, segmentation, optical flow, stereo matching, 3D reconstruction, captioning, video classification, etc.\n",
    "* <b>Text datasets</b>: text classification, language modeling, machine translation, tagging, question answering, etc.\n",
    "* <b>Audio datasets</b>: speaker verification, music genre recognition, emotion recognition, source separation, etc.\n",
    "\n",
    "More on TorchData project (Beta): https://pytorch.org/data/beta/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89dd5f-3156-4883-be73-47529b7bacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CIFAR10(root='./cifar10',\n",
    "                                       train=True,  # False for test set\n",
    "                                       download=True,\n",
    "                                       transform=ToTensor())  # more on data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b3335-2f1b-425b-9f3e-70a1a5e73780",
   "metadata": {},
   "source": [
    "Visualize a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37a696-56e3-4357-8971-e237361a1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "idx = np.random.randint(0, num_samples)\n",
    "sample, label = dataset[idx] # sample, label\n",
    "plt.imshow(sample.numpy().transpose(1, 2, 0))\n",
    "plt.title(dataset.classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aaadc4-b9d7-4c81-aa8e-331d4a46aa33",
   "metadata": {},
   "source": [
    "<b>Conclusions:</b> <br>\n",
    "A PyTorch dataset object behave like a Python iterable \n",
    "* We can index elements\n",
    "* We can iterate over it one element at a time \n",
    "\n",
    "It acts as a datastore: \n",
    "* Each element of the list consists of (sample, label) pair\n",
    "* We need a different mechanism to pass the samples to the training loop i.e. DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69ee0f-f112-40d6-aeeb-e09e0e6d20c9",
   "metadata": {},
   "source": [
    "### Iterator DP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3f09c-93f0-49e6-b1b5-b1c7cc371340",
   "metadata": {},
   "source": [
    "Q: How a data structure is traversed?\n",
    "\n",
    "![data_structure_navigation](../Presentations/assets_data_handling/data_structure_navigation.png)\n",
    "\n",
    "Yields one item at a time without exposing the data structure (dict, list, set, file, tuple, generator - these are already iterable). It doesn't matter if the data structure is linear or not (e.g., tree) => <b>no need to understand the internal representation of the data structure.</b>\n",
    "\n",
    "The pattern involves 3 objects:\n",
    "1. container: aggregate, collection, object whose content is iterable\n",
    "2. item: elements of the container\n",
    "3. iterator: sequential access to items <br>\n",
    "\n",
    "It powers for-loops and list comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae62d6-e593-417a-848e-e75be816bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe change with a tree but might be too confusing\n",
    "class CustomIterator:\n",
    "    def __init__(self, custom_list):\n",
    "        self._custom_list = custom_list\n",
    "        self._idx = 0\n",
    "\n",
    "    def __iter__(self):  # must implement to be considered iterator\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self._idx < len(self._custom_list):\n",
    "            value = self._custom_list[self._idx]\n",
    "            self._idx += 1\n",
    "            return value\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "class CustomList:\n",
    "    def __init__(self, n=10):\n",
    "        self.elems = range(0, n)\n",
    "        self.len = n\n",
    "    \n",
    "    def __iter__(): # this should return an Iterator object\n",
    "        return CustomListIterator(self)\n",
    "        \n",
    "    def __len__():\n",
    "        return self.len  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2ceb4-bc52-4496-9920-17437664e248",
   "metadata": {},
   "source": [
    "#### Why not combine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa4a94-e925-4bd9-8ca6-79815973be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomListIterator:\n",
    "    def __init__(self, n=10):\n",
    "        self._custom_list = range(0, n)\n",
    "        self._idx = 0\n",
    "        self.len = n\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __setitem__(self, idx, value):\n",
    "        self._custom_list[idx] = value\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._custom_list[idx]\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._idx < len(self._custom_list):\n",
    "            value = self._custom_list[self._idx]\n",
    "            self._idx += 1\n",
    "            return value\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfbdfd-5a53-4084-9889-d7c0fff224c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_list_iter = CustomListIterator(10)\n",
    "for x in custom_list_iter:\n",
    "    print(x+1, \"/\", len(custom_list_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df341b-d0a4-42e7-bf1c-8191e8c7165f",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ddf3e2-3c23-4f3d-baab-27171e387949",
   "metadata": {},
   "source": [
    "Function that returns an iterator that produces a (potentially large) sequence of values when iterated over that would otherwise not fit into memory at once.\n",
    "\n",
    "Iterables need to be stored in memory but you have more flexibility over the state.\n",
    "\n",
    "It preserves state between 2 yield calls.\n",
    "\n",
    "yield = return Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eeb0f3-32d7-4523-8e8c-d4fafb6e8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(n):\n",
    "    # n can be very large\n",
    "    cnt = 0\n",
    "    while cnt < n:\n",
    "        yield cnt\n",
    "        cnt += 1\n",
    "        \n",
    "for value in my_generator(2):\n",
    "    print(value)\n",
    "    \n",
    "generator = my_generator(2)\n",
    "print(next(generator))\n",
    "print(next(generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23cb70d-fd8c-4277-9898-28e7e61d9498",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d8cc5-70d7-4f74-9ed7-b89eec55b630",
   "metadata": {},
   "source": [
    "When implementing a torch.utils.data.Dataset one should override:\n",
    "* <b>\\_\\_len__</b> so len(dataset) returns the total number of elements \n",
    "* <b>\\_\\_getitem__</b> to support indexing over elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feae706-e7f6-4bd8-9ea3-f8394f63c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDatasetV1(th.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        # this would be read from a file during iteration\n",
    "        # here we would just pass some paths\n",
    "        self.images = dataset.data\n",
    "        self.labels = dataset.targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image, label =  self.images[idx], self.labels[idx] \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fba8d-ef96-493e-9ce7-d152129fea30",
   "metadata": {},
   "source": [
    "PyTorch supports 2 types of datasets:\n",
    "* map-style (overrides Dataset & implements \\_\\_getitem()__ and \\_\\_len__())\n",
    "* iterable-style (overrides IterableDataset & implements \\_\\_iter__() i.e. there is no notion of key or index): suitable when the batch size depends of the fetched data e.g., data comes from a stream)\n",
    "\n",
    "Also check specializations of these 2 categories: https://pytorch.org/docs/stable/data.html#torch.utils.data.StackDataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c55af4-0182-4db9-86ee-f4925499f4a3",
   "metadata": {},
   "source": [
    "When dealing with large datasets a good practice is:\n",
    "1. Create a dictionary where you gather:\n",
    "* in partition['train'] a list of training IDs (where ID can be the index of image, path of the file, etc.)\n",
    "* in partition['validation'] a list of validation IDs\n",
    "\n",
    "2. Create a dictionary called labels where for each sample ID, the associated label is given by labels[ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd266ac-2532-40b0-b47e-e5707c32859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(th.utils.data.Dataset):\n",
    "  def __init__(self, list_IDs, labels):\n",
    "    self.labels = labels\n",
    "    self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    ID = self.list_IDs[index]\n",
    "   \n",
    "    X = torch.load('data/' + ID + '.pt')\n",
    "    y = self.labels[ID]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcd99f-76e3-406f-8b1c-cdab589ed6cf",
   "metadata": {},
   "source": [
    "### Data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939417c3-ff77-4743-97ca-c6616dcec24c",
   "metadata": {},
   "source": [
    "Goal: manipulate data to make it suitable for training. <br>\n",
    "Use cases:\n",
    "* Preprocessing  <br>\n",
    "* Augmentation <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd03598-9179-4283-b9b6-795b60f426ce",
   "metadata": {},
   "source": [
    "Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b866b3-ecfb-4abd-bfd5-f9e309487f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotate90Left(object):\n",
    "    def __call__(self, image):\n",
    "        return np.rot90(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45f3b7-cdaa-4351-a2a2-0aced06a1910",
   "metadata": {},
   "source": [
    "<b>\\_\\_getitem__</b> can optionally support preprocessing/augmentation functionality => Transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8aa9a-3d2a-4647-97ec-1f2d3139498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDatasetV2(th.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transforms=None, target_transforms=None):\n",
    "        # this would be read from a file during iteration\n",
    "        # here we would just pass some paths\n",
    "        self.images = dataset.data\n",
    "        self.labels = dataset.targets\n",
    "        self.transforms = transforms\n",
    "        self.target_transforms = target_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image, label =  self.images[idx], self.labels[idx] \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        if self.target_transforms:\n",
    "            label = self.target_transforms(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97ae6e-3d95-4e81-8382-cb74d771f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomImageDatasetV2(dataset, transforms=None)\n",
    "plt.imshow(custom_dataset[0][0])\n",
    "plt.show()\n",
    "custom_dataset = CustomImageDatasetV2(dataset, transforms=Rotate90Left())\n",
    "plt.imshow(custom_dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d65752-05f0-41e1-8e58-0231bbac10b7",
   "metadata": {},
   "source": [
    "Composing Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358aca3-3417-43b4-aabf-b9de4e7403c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "# faster and arbitrary input structures like dicts, lists, tuples\n",
    "# Also check:  https://albumentations.ai/\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    # preprocessing\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(th.float32, scale=True), \n",
    "    # augmentation,\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ea825-1952-4393-8bd7-a08e1217fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomImageDatasetV2(dataset, transforms=None)\n",
    "plt.imshow(custom_dataset[0][0])\n",
    "plt.show()\n",
    "custom_dataset = CustomImageDatasetV2(dataset, transforms=transforms)\n",
    "plt.imshow(custom_dataset[0][0].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce148c5-65e3-4115-8f72-07067d92a5a8",
   "metadata": {},
   "source": [
    "During training vs Before training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee434a-280f-471c-b8ef-47a710099299",
   "metadata": {},
   "source": [
    "Before training = Fixed amount of augmentation<br>\n",
    "During training = Variable amount of augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25a432-ca0f-4bf1-9baf-480150fbb059",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d956e-e1c7-472a-8b17-7a4c8ed48cf1",
   "metadata": {},
   "source": [
    "A Dataset retrieves samples one at a time. => iterable over samples.\n",
    "\n",
    "During training, a DataLoader will pass samples in minibatches. => iterable over batches of samples.\n",
    "\n",
    "Parameters:\n",
    "* object implementing torch.utils.data.Dataset\n",
    "* batch_size\n",
    "* shuffle: reshuffle data every epoch to reduce overfitting\n",
    "* uses Python multiprocessing to speed up data retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844b298-2c5f-45e4-ba2f-c634657f2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = th.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=4,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069dc420-cb87-4c3d-8025-f74b23fea314",
   "metadata": {},
   "source": [
    "#### Inner workings of DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d373d-4063-4026-9573-f981ed42fa7b",
   "metadata": {},
   "source": [
    "Multiple components:\n",
    "* Generator: yields batches of data\n",
    "* Collator: formulated as collate_fn combines samples into batches\n",
    "* Sampler: sequential or shuffled sampler will be constructed based on shuffle argument - select indices of samples to be batched\n",
    "\n",
    "Example for CIFAR10:\n",
    "Each sample is a tuple of (image, label). \n",
    "The Sampler will select the indices of the tuples to be batched. i.e. number of batches lists of indices.\n",
    "These lists are then combined into batches by the collate_fn function => list of tensors with first dimension=batch_size. \n",
    "If initially, for a batch we had a list of tuples in the end we will have a tuple of batched images and batched labels.\n",
    "DataLoader then yields batched samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28f829-3808-4ac9-b034-9a7f6410b41e",
   "metadata": {},
   "source": [
    "1. Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ea2c8-b9ab-4081-8cf7-c72aa2ca71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(th.utils.data.BatchSampler(th.utils.data.RandomSampler(range(10)), batch_size=3, drop_last=False))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b39d0-b7bf-43ba-8530-86cd6eb70918",
   "metadata": {},
   "source": [
    "2. (Default) Collator\n",
    "* prepends the batch dimension\n",
    "* automatically converts Numpy and Python data into PyTorch tensors\n",
    "* preserves the data structure (e.g., if each sample is a dict, the output will be a dict; same for list and tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7f616-bf59-45cd-b6f4-b3b77e6926eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.utils.data.default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8742226f-60e4-42ad-95e1-4841035af34e",
   "metadata": {},
   "source": [
    "3. Generator\n",
    "\n",
    "Roughly this is what happens (not sure about the generator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1139e-612d-42c2-9e94-98e2b25dccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset, batch_sampler, collate_fn):\n",
    "    for indices in batch_sampler:\n",
    "        yield collate_fn([dataset[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13760f-ce40-433c-8177-038dfc2fa262",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = data_loader(dataset,\n",
    "                    th.utils.data.BatchSampler(th.utils.data.RandomSampler(range(len(dataset))), batch_size=2048, drop_last=False),\n",
    "                    th.utils.data.default_collate)\n",
    "\n",
    "for batch in loader:\n",
    "    img, labels = batch\n",
    "    print(img.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc18415-f300-4223-ad91-ed1feacad1e4",
   "metadata": {},
   "source": [
    "#### Multiprocessing in DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b13028-5e69-40cd-96cf-ffb08bca4e0d",
   "metadata": {},
   "source": [
    "GIL prevents true parallelization -->  computation/training code is blocked by data loading code --> setting num_workers > 0 switches to multi-process data loading\n",
    "\n",
    "Two strategies:\n",
    "\n",
    "1. Single-process data loading (default): data fetching is done in the same process a DataLoader is initialized (e.g., main). It may be preffered when datasets are small.\n",
    "2. Multi-process data loading: num_workers processes are created each of which gets dataset, collate_fn and worker_init_fn => fetch, transforms & collate run in each worker. Only the workers will retrieve data, main process won't. Batch indices are generated by the batch sampler in the main process and sent to each worker while the main process waits until the batch is retrieved by the assigned worker --> can lead to high I/O load due to these exchanges between processes and high GPU memeory consumption as we load more data at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f63434-2a0b-4a53-b287-d06c7dc2464d",
   "metadata": {},
   "source": [
    "On windows you might get an error: <br>\n",
    "solution - not respawn the processes each epoch: persistent_workers=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19ebf6-17d6-4a5f-a722-316e6e40524b",
   "metadata": {},
   "source": [
    "### Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1c3cf-f53d-45fe-b42d-1bc66af35bf8",
   "metadata": {},
   "source": [
    "* Numpy Archives\n",
    "* Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94608e-7496-4ec4-beae-5e61f58a86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDatasetV3(th.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transforms=None, target_transforms=None):\n",
    "        # this would be read from a file during iteration\n",
    "        # here we would just pass some paths\n",
    "        self.images = dataset.data\n",
    "        self.labels = dataset.targets\n",
    "        self.transforms = transforms\n",
    "        self.target_transforms = target_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image, label =  self.images[idx], self.labels[idx] \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        if self.target_transforms:\n",
    "            label = self.target_transforms(label)\n",
    "        return image, label\n",
    "\n",
    "    def save_data(self, path):\n",
    "        if isinstance(self.images, np.ndarray):\n",
    "            # save for single array -> .npy\n",
    "            # savez for multiple arrays -> .npz\n",
    "            np.savez(path, images=self.images, labels=self.labels)\n",
    "        else:\n",
    "            # slower\n",
    "            # dumps is to represent as byte object\n",
    "            with open(path, \"wb\") as file:\n",
    "                pickle.dump({\"images\": self.images, \"labels\": self.labels}, file)\n",
    "        \n",
    "    def load_data(self, path):\n",
    "        if os.path.exists(path+\".npz\"):\n",
    "            npz = np.load(path+\".npz\")\n",
    "            self.images, self.labels = npz['images'], npz['labels'] \n",
    "        else:\n",
    "            with open(path, \"rb\") as file:\n",
    "                pkl = pickle.load(file)\n",
    "            self.images, self.labels = pkl['images'], pkl['labels'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd2470-2409-4b3f-99b9-86b85fd8a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomImageDatasetV3(dataset)\n",
    "custom_dataset.save_data(\"ds_v3\")\n",
    "custom_dataset.load_data(\"ds_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88021b3-0d1b-46b9-a574-751d31a60524",
   "metadata": {},
   "source": [
    "### End-to-end example: Adaptive Cruise Control (ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7e69f-01b0-4ade-946c-1cf3014bc0a5",
   "metadata": {},
   "source": [
    "![acc_explained.png](../Presentations/assets_data_handling/acc_explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f9333-ce42-4dd5-a191-c584ee522f9e",
   "metadata": {},
   "source": [
    "### ACC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90c9d5-7903-4e06-ae5a-98b5e5f698f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"../Presentations/assets_data_handling/ACC\"):\n",
    "    if files:\n",
    "        print(root, \":\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a53e0-6e7a-4f9f-a70f-413cd28a371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=\"../Presentations/assets_data_handling/ACC\")\n",
    "\n",
    "print(len(dataset))\n",
    "print_details = lambda x: print(x[0].shape, x[1])\n",
    "\n",
    "print_details(dataset[0])\n",
    "print_details(dataset[78])\n",
    "\n",
    "idx = np.random.randint(0, len(dataset))\n",
    "sample = dataset[idx][0]\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42f9c4-aa66-48a8-84e8-13d519b7451f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2792f-d84a-4abd-adfc-90e36c830827",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "NUM_EPOCHS = 50\n",
    "device = (\"cuda\" if th.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99e1a4-ce62-46c3-b833-958c8fb12754",
   "metadata": {},
   "source": [
    "### Data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d63ed-c7fb-4d8e-8216-734abb96a67c",
   "metadata": {},
   "source": [
    "![hsv_space.png](../Presentations/assets_data_handling/hsv_space.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1849e2-30d7-4c12-bb59-dc4a9f93d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighlightRoad:\n",
    "    def __init__(self, intensity_factor=2):\n",
    "        self.intensity_factor=intensity_factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # img is a Pillow image\n",
    "        \n",
    "        img_hsv = np.array(img.convert('HSV'))\n",
    "        val_min = 0.2  \n",
    "        val_max = 0.8  \n",
    "        sat_max = 0.3\n",
    "        saturation_values = img_hsv[:, :, 1] / 255.0\n",
    "        hue_values = img_hsv[:, :, 2] / 255.0 \n",
    "        gray_mask = np.logical_and(hue_values >= val_min, hue_values <= val_max)\n",
    "        gray_mask = np.logical_and(gray_mask, saturation_values <= sat_max)\n",
    "\n",
    "        img_hsv[:, :, 2] = np.where(gray_mask, np.minimum(255, img_hsv[:, :, 2] * self.intensity_factor), img_hsv[:, :, 2])\n",
    "        img_rgb = Image.fromarray(img_hsv, mode='HSV').convert('RGB')\n",
    "        return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e77dc2-6755-4175-a581-bc45de3f8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample)\n",
    "plt.show()\n",
    "sample2 = HighlightRoad()(sample)\n",
    "plt.imshow(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae2009-f49d-4d25-9d07-21eca9fbe5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    return th.tensor([x])\n",
    "\n",
    "def to_one_hot(y):\n",
    "    return th.nn.functional.one_hot(y, num_classes=2)\n",
    "\n",
    "def simple_norm(x):\n",
    "    return x / 255.0\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    HighlightRoad(),\n",
    "    # preprocessing\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(th.float32, scale=True), \n",
    "    v2.Resize((128, 128), antialias=True),\n",
    "    v2.Lambda(simple_norm),\n",
    "    # augmentation,\n",
    "    v2.RandomApply(transforms=[v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 0.1))], p=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "target_transforms = v2.Compose([\n",
    "    v2.Lambda(to_tensor),\n",
    "    # v2.Lambda(to_one_hot),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e036ca3-80b9-4a7a-9f74-b693e9999a38",
   "metadata": {},
   "source": [
    "More examples here: https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f4ce8-84e5-4d0e-8f9f-e1c9730bc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=\"../Presentations/assets_data_handling/ACC\",\n",
    "                                           transform=transforms,\n",
    "                                           target_transform=target_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735f9cd-6a5c-403c-9f6a-e37dad1c0e19",
   "metadata": {},
   "source": [
    "### Train/Val/[Test] Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84df940-27b0-4b2d-a3a8-2f76c7320a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    train_set, val_set = th.utils.data.random_split(dataset, [90, 10])\n",
    "    val_labels = [x[1].item() for x in val_set]\n",
    "    print(val_labels)\n",
    "    if sum(val_labels) == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd62e3f-26e5-4a83-a090-25b1632ca711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = th.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataloader = th.utils.data.DataLoader(val_set,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09661a-8a7c-4c2a-957f-fef18beef9d9",
   "metadata": {},
   "source": [
    "### Epoch-level utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02883af2-4724-42c2-a838-4cf1a45015f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader):\n",
    "    avg_train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_train_loss += loss.item()\n",
    "    return avg_train_loss/len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(eval_dataloader):\n",
    "    avg_val_loss = 0.0\n",
    "    with th.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            avg_val_loss += criterion(outputs, labels.float()).item()\n",
    "    return avg_val_loss/len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad72a0c-3940-40f0-895e-70f3d4dd4d75",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5f84a-9d2f-4143-bc4e-819daa3893a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(th.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = th.nn.Conv2d(3, 16, 3)\n",
    "        self.pool1 = th.nn.MaxPool2d(4, 4)\n",
    "        self.bn1 = th.nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = th.nn.Conv2d(16, 32, 3)\n",
    "        self.pool2 = th.nn.MaxPool2d(4, 4)\n",
    "        self.bn2 = th.nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = th.nn.Conv2d(32, 64, 3)\n",
    "        self.pool3 = th.nn.MaxPool2d(4, 4)\n",
    "        self.bn3 = th.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # self.gap = th.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = th.nn.Flatten()\n",
    "        \n",
    "        self.fc = th.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = th.nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = th.nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = th.nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = self.gap(x)        \n",
    "        # x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = th.nn.functional.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = th.nn.BCELoss()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d63cc9-aa08-4fbf-a398-5aa07dc2b1cf",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8cb563-c24b-4c7c-8230-5c7febb9437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train(train_dataloader)\n",
    "    val_loss = evaluate(val_dataloader)\n",
    "    print(f\"Epoch \\t {epoch} train loss \\t {train_loss} val loss \\t {val_loss}\")\n",
    "\n",
    "    if (val_loss-train_loss)/val_loss > 0.5:\n",
    "        cnt += 1\n",
    "    else:\n",
    "        cnt = 0\n",
    "        \n",
    "    if cnt == 3:\n",
    "        print(\"Early stopping due overfitting\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757ac44-11ee-4ed4-bfd1-d3d9f3661213",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abd5de-332d-4795-9423-2af39bc6c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    idx = np.random.randint(0, len(val_set))\n",
    "    img, gt = val_set[idx]\n",
    "    img = img.to(device)\n",
    "    with th.no_grad():\n",
    "        pred = model(img[None, ...])[0]\n",
    "    plt.imshow(img.cpu().permute(1, 2, 0).numpy() * 255.0)\n",
    "    plt.title(f\"Pred prob  {round(pred.item(), 2)}  GT {gt.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9880a-e045-4134-a649-9dc3e67ccbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e0127-c766-4402-8edf-1e4f56316613",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "* https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "* https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "* https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "* https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a12bc6-a64d-4f40-98a1-6b1bd36faee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582b0ae-f11e-4f4d-a905-ebbbb4fe5f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_ws",
   "language": "python",
   "name": "pt_ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
